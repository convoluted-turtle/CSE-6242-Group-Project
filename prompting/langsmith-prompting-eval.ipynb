{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langsmith.evaluation._runner import evaluate\n",
    "from langsmith.evaluation.integrations import LangChainStringEvaluator\n",
    "from langchain.evaluation.qa.eval_chain import CotQAEvalChain\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "from langchain import prompts\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "\n",
    "db_faiss = FAISS.load_local(\"/Users/justinvhuang/Desktop/CSE-6242-Group-Project/app/faiss_anime_index_v3\", embeddings = embedding_function)\n",
    "retriever = db_faiss.as_retriever(search_kwargs={\"k\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load API key from config.yaml\n",
    "with open(\"/Users/justinvhuang/Desktop/CSE-6242-Group-Project/app/config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "api_key = config[\"api_key\"]\n",
    "import google.generativeai as genai\n",
    "userdata = {\"GOOGLE_API_KEY\": api_key}\n",
    "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "\n",
    "# Generation\n",
    "generator = prompts.ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Respond using the following documents as context:\\n{documents}\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ") | ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0,google_api_key = GOOGLE_API_KEY,convert_system_message_to_human=True)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | generator | StrOutputParser()\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"Who is fririen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0,google_api_key = GOOGLE_API_KEY,convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA\n",
    "inputs = [\n",
    "    \"Who is Fririen\",\n",
    "    \"What are good pirate anime?\",\n",
    "    \"What are some good kid anime?\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"Use RunnablePassthrough. from langchain_core.runnables import RunnableParallel, RunnablePassthrough; from langchain_core.prompts import ChatPromptTemplate; from langchain_openai import ChatOpenAI; prompt = ChatPromptTemplate.from_template('Tell a joke about: {input}'); model = ChatOpenAI(); runnable = ({'input' : RunnablePassthrough()} | prompt | model); runnable.invoke('flowers')\",\n",
    "    \"Use StrOutputParser. from langchain_openai import ChatOpenAI; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.output_parsers import StrOutputParser; prompt = ChatPromptTemplate.from_template('Tell me a short joke about {topic}'); model = ChatOpenAI(model='gpt-3.5-turbo') #gpt-4 or other LLMs can be used here; output_parser = StrOutputParser(); chain = prompt | model | output_parser\",\n",
    "    \"Use RunnableLambda with itemgetter to extract the relevant key. from operator import itemgetter; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.runnables import RunnableLambda; from langchain_openai import ChatOpenAI; def length_function(text): return len(text); chain = ({'prompt_input': itemgetter('foo') | RunnableLambda(length_function),} | prompt | model); chain.invoke({'foo':'hello world'})\"\n",
    "]\n",
    "\n",
    "# Dataset\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true' # enables tracing \n",
    "os.environ['LANGCHAIN_API_KEY'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"RAG_ANIME\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about ANIME.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-e84562f5' at:\n",
      "https://smith.langchain.com/o/ee2bfa72-1f76-5322-9029-9ae994e50470/datasets/ee970bc4-f988-4209-8f64-7a2d348d0771/compare?selectedSessions=aed5058f-6bb3-4ade-b82e-8661c9b4fb51\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4c816b5a054f05b35c6ac95603eff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connection error caused failure to POST https://api.smith.langchain.com/runs  in LangSmith API. Please confirm your internet connection.. ConnectTimeout(MaxRetryError(\"HTTPSConnectionPool(host='api.smith.langchain.com', port=443): Max retries exceeded with url: /runs (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fbcdeaf5fc0>, 'Connection to api.smith.langchain.com timed out. (connect timeout=10.0)'))\"))\n",
      "Error in LangChainTracer.on_chain_start callback: LangSmithConnectionError('Connection error caused failure to POST https://api.smith.langchain.com/runs  in LangSmith API. Please confirm your internet connection.. ConnectTimeout(MaxRetryError(\"HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fbcdeaf5fc0>, \\'Connection to api.smith.langchain.com timed out. (connect timeout=10.0)\\'))\"))')\n"
     ]
    }
   ],
   "source": [
    "# Evaluators\n",
    "qa_evalulator = [LangChainStringEvaluator(\"cot_qa\", config={\"llm\":ChatGoogleGenerativeAI(model=\"gemini-pro\", \n",
    "                                                                                         temperature=0,google_api_key = GOOGLE_API_KEY,convert_system_message_to_human=True)})]\n",
    "dataset_name = \"RAG_ANIME\"\n",
    "\n",
    "def predict(example: dict):\n",
    "    return {\"answer\": rag_chain.invoke(example[\"question\"])}\n",
    "    \n",
    "experiment_results = evaluate(\n",
    "    predict,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"rag-qa-oai\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "      \"variant\": \"ANIME context, gemini\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
